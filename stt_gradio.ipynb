{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stt_gradio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNPu9IPLo6nsB7QxPsrFnv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsnam95/class2022Spring/blob/main/stt_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo5ut1PNcpoF"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install gradio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[0][\"audio\"][\"array\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTOKbP_ccq2L",
        "outputId": "1e74a428-5981-4cee-e76b-b042a46fce87"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(93680,)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcription[0]"
      ],
      "metadata": {
        "id": "QmdxBkc5d4qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model and tokenizer\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    \n",
        "# load dummy dataset and read soundfiles\n",
        "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "\n",
        "# tokenize\n",
        "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
        "\n",
        "# retrieve logits\n",
        "logits = model(input_values).logits\n",
        "\n",
        "# take argmax and decode\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)"
      ],
      "metadata": {
        "id": "cCjYRKPqcq4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stt (audio):\n",
        "  import numpy as np\n",
        "  sig = audio[1]/np.max(np.abs(audio[1]))\n",
        "\n",
        "  # load model and tokenizer\n",
        "  processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "  model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "  # tokenize\n",
        "  input_values = processor(sig, return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
        "\n",
        "  # retrieve logits\n",
        "  logits = model(input_values).logits\n",
        "\n",
        "  # take argmax and decode\n",
        "  predicted_ids = torch.argmax(logits, dim=-1)\n",
        "  transcription = processor.batch_decode(predicted_ids)\n",
        "  return transcription[0]"
      ],
      "metadata": {
        "id": "Eh_44HuIcq7t"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(stt, inputs = [\"mic\"], outputs = [\"text\"])\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yO8QuZM_cq-C",
        "outputId": "3d2d574b-4e0f-4892-b387-b668ea1a65de"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Your interface requires microphone or webcam permissions - this may cause issues in Colab. Use the External URL in case of issues.\n",
            "Running on public URL: https://42692.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fbdec0a4f10>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://42692.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/routes.py\", line 269, in predict\n",
            "    output = await run_in_threadpool(app.launchable.process_api, body, username)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\n",
            "    return await anyio.to_thread.run_sync(func, *args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/anyio/to_thread.py\", line 29, in run_sync\n",
            "    limiter=limiter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/interface.py\", line 573, in process_api\n",
            "    prediction, durations = self.process(raw_input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/interface.py\", line 616, in process\n",
            "    processed_input, return_duration=True\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/interface.py\", line 531, in run_prediction\n",
            "    prediction = predict_fn(*processed_input)\n",
            "  File \"<ipython-input-47-e24b7efa74da>\", line 10, in stt\n",
            "    input_values = processor(sig, 48000, return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py\", line 73, in __call__\n",
            "    return self.current_processor(*args, **kwargs)\n",
            "TypeError: __call__() got multiple values for argument 'padding'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<fastapi.applications.FastAPI at 0x7fbdf8f52b50>,\n",
              " 'http://127.0.0.1:7863/',\n",
              " 'https://42692.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hzkQ6d6ncrBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-CrqVYrrcrEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}