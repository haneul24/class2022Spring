{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tts_gradio.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMktOQNUBslgYosLBwXTCDY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsnam95/class2022Spring/blob/main/tts_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pll7e2x_rAvH"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "683Z16BNrBib"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/pytorch/fairseq.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "1KnuDRzAyT_X",
        "outputId": "b9d606f2-51aa-4c3d-ad82-4a6a84448f3d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+47e2798-cp37-cp37m-linux_x86_64.whl size=16170479 sha256=e3d388deadaa6eec28427aaf2e8e818c781880ffc2376ec75ef13a69d2f8b868\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p1rqvr85/wheels/3b/46/38/c078e70b6a4e984d7214431a0098b99b5b593a9e22dc028792\n",
            "Successfully built fairseq\n",
            "Installing collected packages: omegaconf, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.1.1\n",
            "    Uninstalling omegaconf-2.1.1:\n",
            "      Successfully uninstalled omegaconf-2.1.1\n",
            "  Attempting uninstall: hydra-core\n",
            "    Found existing installation: hydra-core 1.1.1\n",
            "    Uninstalling hydra-core-1.1.1:\n",
            "      Successfully uninstalled hydra-core-1.1.1\n",
            "  Attempting uninstall: fairseq\n",
            "    Found existing installation: fairseq 0.10.2\n",
            "    Uninstalling fairseq-0.10.2:\n",
            "      Successfully uninstalled fairseq-0.10.2\n",
            "Successfully installed bitarray-2.4.1 fairseq-1.0.0a0+47e2798 hydra-core-1.0.7 omegaconf-2.0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fairseq",
                  "hydra",
                  "omegaconf"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-af37c6b4745c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install git+https://github.com/pytorch/fairseq.git'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model_ensemble_and_task_from_hf_hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_model_ensemble_and_task_from_hf_hub' from 'fairseq.checkpoint_utils' (/usr/local/lib/python3.7/dist-packages/fairseq/checkpoint_utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\n",
        "from fairseq.models.text_to_speech.hub_interface import TTSHubInterface"
      ],
      "metadata": {
        "id": "emZ-Inaf4MlF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'how are you?'\n",
        "\n",
        "models, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n",
        "    \"facebook/fastspeech2-en-ljspeech\",\n",
        "    arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False}\n",
        "    )\n",
        "model = models[0]\n",
        "TTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\n",
        "generator = task.build_generator(models, cfg)\n",
        "\n",
        "!pip install g2p_en\n",
        "sample = TTSHubInterface.get_model_input(task, text)\n",
        "wav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uue4QVVuuYgl",
        "outputId": "4497bffe-d565-49fa-d90e-87a2aa6f1f79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-02 15:29:23 | INFO | fairseq.tasks.speech_to_text | dictionary size (vocab.txt): 75\n",
            "2022-04-02 15:29:24 | INFO | fairseq.models.text_to_speech.vocoder | loaded HiFiGAN checkpoint from /root/.cache/fairseq/facebook--fastspeech2-en-ljspeech.main.a3e3e5e2e62bb7ca7514b11aa469e9c5b01a20bf/hifigan.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting g2p_en\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from g2p_en) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from g2p_en) (1.21.5)\n",
            "Collecting distance>=0.1.3\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.7/dist-packages (from g2p_en) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.4->g2p_en) (1.15.0)\n",
            "Building wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16276 sha256=53bd15f3eecc5eaccb684a53dbac070d2a11bb4a997886759ff5cd11c32e70cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/10/1b/96fca621a1be378e2fe104cfb0d160bb6cdf3d04a3d35266cc\n",
            "Successfully built distance\n",
            "Installing collected packages: distance, g2p-en\n",
            "Successfully installed distance-0.1.3 g2p-en-2.1.0\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tts(text):\n",
        "  models, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n",
        "      \"facebook/fastspeech2-en-ljspeech\",\n",
        "      arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False}\n",
        "      )\n",
        "  model = models[0]\n",
        "  TTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\n",
        "  generator = task.build_generator(models, cfg)\n",
        "\n",
        "  !pip install g2p_en\n",
        "  sample = TTSHubInterface.get_model_input(task, text)\n",
        "  wav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\n",
        "\n",
        "  return (rate, wav)"
      ],
      "metadata": {
        "id": "VC_l2NeerUWv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(tts, inputs = [\"text\"], outputs = [\"audio\"])\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qju6jorvrR-E",
        "outputId": "afffa343-2b87-47b8-e04a-e77ab3bea9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-02 15:30:30 | INFO | paramiko.transport | Connected (version 2.0, client OpenSSH_7.6p1)\n",
            "2022-04-02 15:30:31 | INFO | paramiko.transport | Authentication (publickey) successful!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on public URL: https://12219.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f1ba6c13f10>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://12219.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-02 15:30:48 | INFO | fairseq.tasks.speech_to_text | dictionary size (vocab.txt): 75\n",
            "2022-04-02 15:30:49 | INFO | fairseq.models.text_to_speech.vocoder | loaded HiFiGAN checkpoint from /root/.cache/fairseq/facebook--fastspeech2-en-ljspeech.main.a3e3e5e2e62bb7ca7514b11aa469e9c5b01a20bf/hifigan.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: g2p_en in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.7/dist-packages (from g2p_en) (3.2.5)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from g2p_en) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.7/dist-packages (from g2p_en) (1.21.5)\n",
            "Requirement already satisfied: distance>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from g2p_en) (0.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.4->g2p_en) (1.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/routes.py\", line 269, in predict\n",
            "    output = await run_in_threadpool(app.launchable.process_api, body, username)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/starlette/concurrency.py\", line 39, in run_in_threadpool\n",
            "    return await anyio.to_thread.run_sync(func, *args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/anyio/to_thread.py\", line 29, in run_sync\n",
            "    limiter=limiter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/interface.py\", line 573, in process_api\n",
            "    prediction, durations = self.process(raw_input)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/interface.py\", line 622, in process\n",
            "    for i, output_component in enumerate(self.output_components)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/interface.py\", line 622, in <listcomp>\n",
            "    for i, output_component in enumerate(self.output_components)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/outputs.py\", line 463, in postprocess\n",
            "    processing_utils.audio_to_file(sample_rate, data, file.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/processing_utils.py\", line 134, in audio_to_file\n",
            "    data = convert_to_16_bit_wav(data)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gradio/processing_utils.py\", line 171, in convert_to_16_bit_wav\n",
            "    raise ValueError(\"Audio data cannot be converted to \" \"16-bit int format.\")\n",
            "ValueError: Audio data cannot be converted to 16-bit int format.\n"
          ]
        }
      ]
    }
  ]
}